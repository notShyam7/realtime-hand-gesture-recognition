{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/notShyam7/realtime-hand-gesture-recognition/blob/main/HandGesture.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N-2XOEBimA3f"
      },
      "outputs": [],
      "source": [
        "!mkdir kaggle\n",
        "!cp kaggle.json ~/.kaggle/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2pW7kTrvk2G0"
      },
      "outputs": [],
      "source": [
        "!kaggle datasets download -d gti-upm/leapgestrecog"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LwzstnMbm2xF"
      },
      "outputs": [],
      "source": [
        "!chmod 600 /root/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nzzsak5GnK-8"
      },
      "outputs": [],
      "source": [
        "!unzip leapgestrecog.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QbqfJ4iPzGHe"
      },
      "outputs": [],
      "source": [
        "!pip install opencv-python tensorflow numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6BYs-bSECNxG"
      },
      "outputs": [],
      "source": [
        "# colab mount\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dGnCPlvx955y"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2dlzC96B-pkb"
      },
      "outputs": [],
      "source": [
        "# Define your dataset directories\n",
        "dataset_directories = [\"/content/leapGestRecog\", \"/content/leapgestrecog\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HihSm1og-vjD"
      },
      "outputs": [],
      "source": [
        "# Function to load and preprocess the dataset from multiple directories\n",
        "def load_and_preprocess_data(dataset_directories):\n",
        "    images = []\n",
        "    labels = []\n",
        "\n",
        "    for dataset_directory in dataset_directories:\n",
        "        for class_folder in os.listdir(dataset_directory):\n",
        "            class_dir = os.path.join(dataset_directory, class_folder)\n",
        "\n",
        "            if os.path.isdir(class_dir):\n",
        "                for gesture_folder in os.listdir(class_dir):\n",
        "                    gesture_dir = os.path.join(class_dir, gesture_folder)\n",
        "\n",
        "                    if os.path.isdir(gesture_dir):\n",
        "                        for image_file in os.listdir(gesture_dir):\n",
        "                            if image_file.endswith(\".png\"):\n",
        "                                # Load the image\n",
        "                                img = cv2.imread(os.path.join(gesture_dir, image_file))\n",
        "                                img = cv2.resize(img, (64, 64))  # Resize to a consistent size\n",
        "                                img = img / 255.0  # Normalize pixel values\n",
        "                                images.append(img)\n",
        "\n",
        "                                # Extract class label from the class folder name (e.g., \"00\")\n",
        "                                label = int(class_folder)\n",
        "                                labels.append(label)\n",
        "\n",
        "    images = np.array(images)\n",
        "    labels = np.array(labels)\n",
        "\n",
        "    return images, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BILuSgGC-zjE"
      },
      "outputs": [],
      "source": [
        "# Load and preprocess the dataset from both directories\n",
        "train_images, train_labels = load_and_preprocess_data(dataset_directories)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A58RxpMQ_DZ6"
      },
      "outputs": [],
      "source": [
        "# Split the data into training and testing sets\n",
        "test_size = 0.2  # Adjust the ratio as needed\n",
        "train_images, test_images, train_labels, test_labels = train_test_split(train_images, train_labels, test_size=test_size, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CCp-9sPl3d1U"
      },
      "outputs": [],
      "source": [
        "# Verify the shapes of the resulting datasets\n",
        "print(\"Number of training images:\", len(train_images))\n",
        "print(\"Number of training labels:\", len(train_labels))\n",
        "print(\"Number of testing images:\", len(test_images))\n",
        "print(\"Number of testing labels:\", len(test_labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nfijXdmbA_OQ"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.utils import to_categorical"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NNAGa5R9-D-3"
      },
      "outputs": [],
      "source": [
        "# Determine the number of unique labels in the training data\n",
        "num_classes = len(np.unique(train_labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PGr2KdZaBB6b"
      },
      "outputs": [],
      "source": [
        "# Convert labels to one-hot encoded format\n",
        "train_labels = to_categorical(train_labels, num_classes=num_classes)\n",
        "test_labels = to_categorical(test_labels, num_classes=num_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ttka01ix93aC"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fbMTqwcH-HKP"
      },
      "outputs": [],
      "source": [
        "# Define your CNN model\n",
        "model = keras.Sequential([\n",
        "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 3)),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dense(num_classes, activation='softmax')  # Use num_classes here\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AV5s8ShS-Jse"
      },
      "outputs": [],
      "source": [
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lV996lTq2vsy"
      },
      "outputs": [],
      "source": [
        "# Train the model\n",
        "model.fit(train_images, train_labels, epochs=10, validation_data=(test_images, test_labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DeS6DS7_Cm4t"
      },
      "outputs": [],
      "source": [
        "model.save('/content/gdrive/MyDrive/handgesture/gestures.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hnvkDzTCkpRv",
        "outputId": "6dfc3a76-2813-4f1b-de8c-db3a67e12b75"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/Users/ghanshyam/Desktop/handgesture/Desktop/handgesture/gestures.h5\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "print(os.path.abspath('Desktop/handgesture/gestures.h5'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KLw7mvxUkpRw"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 533,
          "referenced_widgets": [
            "50e80d94f8e04edf939bbf6e3ed98f96",
            "a9a5c28599084aeaaf9f3d274af4092e",
            "8f81dd96475340f49a0b762c59675539",
            "ced17e2d421a42b883af4ad300c59a67",
            "a4702a01335145b69433c4d710bd6137"
          ]
        },
        "id": "0BCk6n5u3Blz",
        "outputId": "21383125-3811-48d1-fe60-6f50c5c00a8e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
            "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 57ms/step\n",
            "1/1 [==============================] - 0s 12ms/step\n",
            "1/1 [==============================] - 0s 12ms/step\n",
            "1/1 [==============================] - 0s 12ms/step\n",
            "1/1 [==============================] - 0s 12ms/step\n",
            "1/1 [==============================] - 0s 12ms/step\n",
            "1/1 [==============================] - 0s 11ms/step\n",
            "1/1 [==============================] - 0s 11ms/step\n",
            "1/1 [==============================] - 0s 12ms/step\n",
            "1/1 [==============================] - 0s 11ms/step\n",
            "1/1 [==============================] - 0s 11ms/step\n",
            "1/1 [==============================] - 0s 11ms/step\n",
            "1/1 [==============================] - 0s 11ms/step\n",
            "1/1 [==============================] - 0s 11ms/step\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# Load your trained model\n",
        "model = tf.keras.models.load_model('/Users/ghanshyam/Desktop/handgesture/gestures.h5')  # Update with the path to your model file\n",
        "\n",
        "# Define a function to preprocess the frame for model input\n",
        "def preprocess_frame(frame):\n",
        "    frame = cv2.resize(frame, (64, 64))\n",
        "    frame = frame / 255.0  # Normalize pixel values\n",
        "    return frame\n",
        "\n",
        "# OpenCV VideoCapture\n",
        "cap = cv2.VideoCapture(1)\n",
        "\n",
        "while True:\n",
        "    try:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            continue\n",
        "\n",
        "        # Preprocess the frame\n",
        "        preprocessed_frame = preprocess_frame(frame)\n",
        "\n",
        "        # Make predictions\n",
        "        predictions = model.predict(np.expand_dims(preprocessed_frame, axis=0))\n",
        "\n",
        "        # Find the predicted gesture\n",
        "        predicted_gesture = np.argmax(predictions)\n",
        "\n",
        "        # Display the recognized gesture on the frame\n",
        "        cv2.putText(frame, f'Gesture: {predicted_gesture}', (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
        "\n",
        "        # Display the frame in a window\n",
        "        cv2.imshow('Hand Gesture Recognition', frame)\n",
        "\n",
        "        # Break the loop when the 'q' key is pressed\n",
        "        key = cv2.waitKey(1)\n",
        "        if key == ord('q'):\n",
        "            break\n",
        "    except KeyboardInterrupt:\n",
        "        break\n",
        "\n",
        "# Release the webcam and close OpenCV windows\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-dBB98GkpRx"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bTaCOoa0EYlc"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "50e80d94f8e04edf939bbf6e3ed98f96": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Start Webcam Recognition",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_a9a5c28599084aeaaf9f3d274af4092e",
            "style": "IPY_MODEL_8f81dd96475340f49a0b762c59675539",
            "tooltip": ""
          }
        },
        "8f81dd96475340f49a0b762c59675539": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "a4702a01335145b69433c4d710bd6137": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a9a5c28599084aeaaf9f3d274af4092e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ced17e2d421a42b883af4ad300c59a67": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ImageModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ImageModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ImageView",
            "format": "jpeg",
            "height": "480",
            "layout": "IPY_MODEL_a4702a01335145b69433c4d710bd6137",
            "width": "640"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}